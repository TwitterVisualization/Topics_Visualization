{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "from tqdm import tqdm_notebook, tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle as pkl\n",
    "from scipy.stats import pearsonr, spearmanr\n",
    "from scipy.ndimage import gaussian_filter1d\n",
    "import matplotlib.dates as mdates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load epi data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "day_flux.pkl       topics.pkl           word2vec_300d.model\r\n",
      "\u001b[0m\u001b[01;34mlang_files\u001b[0m/        trends.pkl           word2vec_300d.model.syn1neg.npy\r\n",
      "texts.txt          trends_raw.pkl       word2vec_300d.model.wv.vectors.npy\r\n",
      "topics_growth.pkl  word2vec_150d.model  word_counts.pkl\r\n"
     ]
    }
   ],
   "source": [
    "ls /mlo-container-scratch/hartley/twitter_covid_insights/insights_All"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/mlo-container-scratch/hartley/twitter_covid_insights_v3/insights_All/day_flux.pkl'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-573c1be1ee89>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# Nb. of tweets per day for normalization\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mday_flux_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'/mlo-container-scratch/hartley/twitter_covid_insights_v3/insights_All/day_flux.pkl'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mday_flux\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpkl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mday_flux_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;31m# Read counts\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/mlo-container-scratch/hartley/twitter_covid_insights_v3/insights_All/day_flux.pkl'"
     ]
    }
   ],
   "source": [
    "ds_path=\"/mlo-container-scratch/hartley/tweets_piped\"\n",
    "files = [f for f in  sorted(os.listdir(ds_path))]\n",
    "piped_dates = [f.split('_')[1].split('.')[0] for f in files]\n",
    "\n",
    "# Nb. of tweets per day for normalization\n",
    "day_flux_path = '/mlo-container-scratch/hartley/twitter_covid_insights_v3/insights_All/day_flux.pkl'\n",
    "day_flux = pkl.load(open(day_flux_path, 'rb'))\n",
    "\n",
    "# Read counts\n",
    "trends_path = '/mlo-container-scratch/hartley/twitter_covid_insights_v3/insights_All/new_trends.pkl'\n",
    "trends = pkl.load(open(trends_path, 'rb'))[0]\n",
    "\n",
    "# Read epi for US\n",
    "epi_df = pd.read_csv('../Data/countries_data.csv')\n",
    "epi_df = epi_df[epi_df.iso_code == 'USA']\n",
    "country_dates = epi_df['date'].values\n",
    "\n",
    "# Overlapping Dates\n",
    "dates = set(piped_dates[110:]) & set(country_dates)\n",
    "trends_to_keep = [True if d in dates else False for d in piped_dates[110:]]\n",
    "\n",
    "# Normalize trends, drop 110 first for stability\n",
    "for key in trends :\n",
    "    trends[key] = (trends[key] / day_flux)[110:][trends_to_keep]\n",
    "    trends[key] /= trends[key].sum()\n",
    "    \n",
    "# Create polarized trends\n",
    "for e in range(int(len(trends) / 3)) :\n",
    "    trends[f'Pol-{e}'] = trends[f'Pos-{e}'] / (trends[f'Pos-{e}'] + trends[f'Neg-{e}'])\n",
    "    trends.pop(f'Pos-{e}', None)\n",
    "    trends.pop(f'Neg-{e}', None)\n",
    "\n",
    "trends = pd.DataFrame(trends)\n",
    "\n",
    "\n",
    "epi_features = ['case',\n",
    "               'death',\n",
    "               'test',\n",
    "               'stringency',\n",
    "               'r_estim']\n",
    "\n",
    "mobility_features = ['transit']\n",
    "\n",
    "weather_features = ['maxtempC', 'mintempC', 'FeelsLikeC', 'humidity', 'pressure']\n",
    "to_correlate = epi_features + mobility_features + weather_features\n",
    "cols_to_keep = [\"date\", \"iso_code\"] + to_correlate\n",
    "\n",
    "# Load epi df for US with relevant dates\n",
    "\n",
    "epi_df = epi_df[epi_df.date.isin(dates)].sort_values('date').reset_index(drop=True)\n",
    "\n",
    "# Normalize counts\n",
    "epi_df[to_correlate] = epi_df[to_correlate].astype(float) / epi_df[to_correlate].sum(axis=0)\n",
    "\n",
    "# Remove and display Nan values\n",
    "#epi_df = epi_df.dropna(axis=\"columns\", thresh = int(0.9*len(epi_df)))\n",
    "#epi_df.fillna(method='ffill', inplace=True)\n",
    "\n",
    "# Convert date column to datetime\n",
    "epi_df['converted_date'] = pd.to_datetime(epi_df['date'], format='%Y-%m-%d')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gaussian Smoothing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'trends' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-999e16429571>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mtopic_nb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'20'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mfrequency\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m14\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrends\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtopic_nb\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'original trend'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgaussian_filter1d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrends\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtopic_nb\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'\\u03C3=3'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgaussian_filter1d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrends\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtopic_nb\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'\\u03C3=5'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'trends' is not defined"
     ]
    }
   ],
   "source": [
    "topic_nb = '20'\n",
    "frequency=14\n",
    "plt.plot(trends[topic_nb], label='original trend')\n",
    "plt.plot(gaussian_filter1d(trends[topic_nb], 3), label='\\u03C3=3')\n",
    "plt.plot(gaussian_filter1d(trends[topic_nb], 5), label='\\u03C3=5')\n",
    "plt.plot(gaussian_filter1d(trends[topic_nb], 10), label='\\u03C3=10')\n",
    "plt.hlines(0.05, 0, len(trends[topic_nb]), linestyle='--', label='event threshold')\n",
    "plt.legend()\n",
    "plt.xticks(np.arange(len(dates))[::frequency], sorted(dates)[::frequency], rotation=90)\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Trend')\n",
    "plt.savefig('gaussian_smoothing.png', dpi=600)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trends by time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'trends' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-fc7ff99f81a9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mhashtags_of_interest\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\"#mentalhealth\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"#delivery\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"#prayer\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mtopic_ids_of_interest\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mis_pol\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'-'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrends\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mcontinuous_thresh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.05\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mtopics_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'/mlo-container-scratch/massemin/twitter_covid_insights_v3/insights_All/topics.pkl'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'trends' is not defined"
     ]
    }
   ],
   "source": [
    "# Find topic ids of the topics that contain the relevant hashtags\n",
    "hashtags_of_interest = [\"#mentalhealth\", \"#delivery\", \"#prayer\"]\n",
    "topic_ids_of_interest = {}\n",
    "is_pol = np.array(['-' in x for x in trends.columns])\n",
    "continuous_thresh = 0.05\n",
    "topics_path = '/mlo-container-scratch/massemin/twitter_covid_insights_v3/insights_All/topics.pkl'\n",
    "topics = pkl.load(open(topics_path, 'rb'))[0]\n",
    "\n",
    "for topic_name in trends:\n",
    "    topic_valid = trends[topic_name].notna()\n",
    "    if '-' in topic_name:\n",
    "        topic_idx = int(topic_name[4:]) \n",
    "        for hashtag in hashtags_of_interest:\n",
    "            if hashtag in topics[topic_idx]:\n",
    "                topic_ids_of_interest[hashtag] = topic_idx\n",
    "                print(f\"Name: {topic_name} Topic id: {topic_idx} Hashes:{'-'.join(topics[topic_idx])}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'#mentalhealth'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-1db4a4b880ab>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;31m# Plot trends for all hashtags of interest\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mhashtag\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mhashtags_of_interest\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m     \u001b[0mplot_topic_trend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhashtag\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-9-1db4a4b880ab>\u001b[0m in \u001b[0;36mplot_topic_trend\u001b[0;34m(topic_name, gaussian_sigma)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mplot_topic_trend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtopic_name\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgaussian_sigma\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0;31m# Plot topic trend alongside epidemiological data.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mtopic_id\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtopic_ids_of_interest\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtopic_name\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mcorrelated_epi_trends\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\"death\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"transit\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"stringency\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mfig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfigsize\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m6\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: '#mentalhealth'"
     ]
    }
   ],
   "source": [
    "def plot_topic_trend(topic_name: str, gaussian_sigma: int = 3):\n",
    "    # Plot topic trend alongside epidemiological data.\n",
    "    topic_id = str(topic_ids_of_interest[topic_name])\n",
    "    correlated_epi_trends = [\"death\", \"transit\", \"stringency\"]\n",
    "    fig = plt.figure(figsize = (10, 6))\n",
    "    ax = fig.add_subplot(1, 1, 1)\n",
    "    ax.xaxis.set_major_formatter(mdates.DateFormatter('%Y-%m'))\n",
    "    ax.xaxis.set_major_locator(mdates.MonthLocator())\n",
    "    for epi_trend in correlated_epi_trends:\n",
    "        ax.plot(epi_df['converted_date'], gaussian_filter1d(epi_df[epi_trend], gaussian_sigma), label=epi_trend.title())\n",
    "    ax.plot(epi_df['converted_date'], gaussian_filter1d(trends[topic_id], gaussian_sigma), label=topic_name, linewidth=4)\n",
    "    ax.xaxis_date()\n",
    "    fig.autofmt_xdate()\n",
    "    plt.xlabel('Date')\n",
    "    plt.ylabel('Trend')\n",
    "    plt.legend()\n",
    "\n",
    "# Plot trends for all hashtags of interest\n",
    "for hashtag in hashtags_of_interest:\n",
    "    plot_topic_trend(hashtag)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute correlations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'trends' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-bcabc90c22b5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;31m# Categorize events\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0mis_pol\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'-'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrends\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0mpeak_events\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrends\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrends\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0.25\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0mcontinuous_events\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrends\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrends\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mcontinuous_thresh\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0mis_pol\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'trends' is not defined"
     ]
    }
   ],
   "source": [
    "corr_method = 'pearson'\n",
    "corr_strength = 0.8\n",
    "continuous_thresh = 0.05\n",
    "blur = True\n",
    "report_path = f'{corr_method}_report_{corr_strength}.txt'\n",
    "country = 'USA'\n",
    "topics_path = '/mlo-container-scratch/massemin/twitter_covid_insights_v3/insights_All/topics.pkl'\n",
    "topics = pkl.load(open(topics_path, 'rb'))[0]\n",
    "\n",
    "# Categorize events\n",
    "is_pol = np.array(['-' in x for x in trends.columns])\n",
    "peak_events = trends.columns[trends.max(axis=0) > 0.25]\n",
    "continuous_events = trends.columns[(trends.max(axis=0) < continuous_thresh) | is_pol]\n",
    "print(f\"Correlating {len(continuous_events)} continuous topics\")\n",
    "\n",
    "scores = []\n",
    "twitter_sigma = 5\n",
    "epi_sigma = 2\n",
    "\n",
    "# Compute the correlation\n",
    "for country_feat in tqdm(to_correlate) :\n",
    "    country_valid = epi_df[country_feat].notna()\n",
    "\n",
    "    for topic_name in continuous_events :\n",
    "\n",
    "        topic_valid = trends[topic_name].notna()\n",
    "        count_values = trends[topic_name].values[country_valid & topic_valid]\n",
    "        country_values = epi_df[country_feat].values[country_valid & topic_valid]\n",
    "\n",
    "        if blur :\n",
    "            count_values = gaussian_filter1d(count_values, twitter_sigma)\n",
    "            country_values = gaussian_filter1d(country_values, epi_sigma)                \n",
    "\n",
    "        if corr_method is \"spearman\" :\n",
    "\n",
    "            # Spearman\n",
    "            corr, pvalue = spearmanr(country_values, count_values)\n",
    "            scores.append((corr, pvalue, topic_name, country_feat))\n",
    "\n",
    "        elif corr_method is \"pearson\" :\n",
    "\n",
    "            # Pearson\n",
    "            corr, pvalue = pearsonr(country_values, count_values)\n",
    "            scores.append((corr, pvalue, topic_name, country_feat))\n",
    "\n",
    "\n",
    "scores = sorted(scores, key = lambda x : x[0])\n",
    "\n",
    "# Generate correlation report\n",
    "to_plot = to_correlate\n",
    "nb_to_plot = 10\n",
    "\n",
    "strong_corrs = 0\n",
    "for e, feature in enumerate(to_plot) :\n",
    "\n",
    "    # Compute specific scores\n",
    "    feat_scores = [s for s in scores if s[-1]==feature]\n",
    "    pos_scores = [s for s in feat_scores if s[0] >= corr_strength]\n",
    "    neg_scores = [s for s in feat_scores[::-1] if s[0] <= -corr_strength]\n",
    "\n",
    "    strong_corrs += len(neg_scores) + len(pos_scores)\n",
    "\n",
    "    # Start writing report\n",
    "    mode = \"w\" if e == 0 else \"a\"\n",
    "\n",
    "    with open(report_path, mode, encoding='utf8') as f :\n",
    "\n",
    "        if e == 0 :\n",
    "            f.write('-'*30 + \"\\n\")\n",
    "            f.write(f\"REPORT FOR {country}\\n\")\n",
    "            f.write('-'*30 + \"\\n\")\n",
    "\n",
    "        f.write(f\"\\n\\n{feature}\\n\".upper())\n",
    "        f.write(f\"Found {len(pos_scores)} strong positive correlations\\n\")\n",
    "        f.write(f\"Found {len(neg_scores)} strong negative correlations\\n\\n\")\n",
    "\n",
    "        if len(pos_scores) > 0 :\n",
    "            f.write(f\"Positive correlations :\\n\")\n",
    "            for pos in pos_scores :\n",
    "                topic_idx = int(pos[2]) if '-' not in pos[2] else int(pos[2][4:])\n",
    "                topic_hashes = \"-\".join(topics[topic_idx])\n",
    "                f.write(f\"\\t{pos[0]:.3}/{pos[1]:.3} : {pos[2]} = {topic_hashes}\\n\")\n",
    "\n",
    "        if len(neg_scores) > 0 :\n",
    "            f.write(f\"\\nNegative correlations :\\n\")\n",
    "            for neg in neg_scores :\n",
    "                topic_idx = int(neg[2]) if '-' not in neg[2] else int(neg[2][4:])\n",
    "                topic_hashes = \"-\".join(topics[topic_idx])\n",
    "                f.write(f\"\\t{neg[0]:.3}/{neg[1]:.3} : {neg[2]} = {topic_hashes}\\n\")\n",
    "\n",
    "\n",
    "print(f\"Found {strong_corrs} strong correlations\")\n",
    "print(f\"Report generated successfully at {report_path}\")\n",
    "\n",
    "pkl.dump(scores, open('/mlo-container-scratch/massemin/twitter_covid_insights_v3/insights_All/corr_scores.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
